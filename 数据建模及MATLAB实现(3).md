# 数据建模及MATLAB实现(二)
随着信息技术的发展和成熟，各行业积累的数据越来越多，因此需要通过数据建模的方法，从看似杂乱的海量数据中找到有用的信息。
## 支持向量机(SVM)
支持向量机(Support Vector Machine,SVM)是新一代的基于统计理论的学习系统。SVM按照监督学习的方式，将训练集按照类别分开，或者是预测新的训练点所对应的类别。
### SVM基本思想
SVM目的是构建一个分割两类的超平面，并且令这个超平面使得两类的分割达到最大化。以一个很大的边缘分隔两个类可以使得期望泛化误差最小化——即当出现新的样本时分类错误的概率尽可能得小。

一般来说，两类之间最中间的一个隔板分类错误概率最小，因此，在SVM中，我们也用两类之间的超平面$\omega^Tx_i+b=0$作为两类的隔板，而$\omega^Tx_i+b=\alpha$与$\omega^Tx_i+b=-\alpha$分别是两类的平行的边界平面。边界平面，即与分类器平面平行且穿过数据集中的至少一个点的超平面。但边界平面的选择有许多种，要让超平面分割的准确，需要让两个边界平面的距离最大化，也就是边缘最大化。“通过SVM学习”的含义就是找到最大化边缘的超平面。
### SVM理论基础
首先，假设存在一个容量为$n$的训练集样本$\{(x_i,y_i),i=1,2,\cdots,n\}$由两个类别组成，若$x_i$属于第一类，则记$y_i=1$;若$x_i$属于第二类，则记$y_i=-1$。

若存在分类超平面：
$$
    \omega^Tx_i+b=0
$$
则能够将样本正确的分为两类，即相同类别的样本都都落在分类超平面的同一侧。即满足
$$
\left\{
\begin{aligned}
    \omega^Tx_i+b&\ge\alpha \ \ \ y_i=1\\
    \omega^Tx_i+b&\le-\alpha \ \ \ y_i=-1\\
    \alpha&>0
\end{aligned}
\right.
$$
两边同除以$\alpha$则可表示为
$$
\left\{
\begin{aligned}
    \omega^Tx_i+b&\ge1 \ \ \ y_i=1\\
    \omega^Tx_i+b&\le-1 \ \ \ y_i=-1
\end{aligned}
\right.
$$
可以综合表达为
$$
y_i(\omega^Tx_i+b)\ge1
$$
而超平面之间的距离，即边缘可表示为
$$
\frac{2}{||\omega^T||}
$$
则规划问题可表示为
$$
    max:\frac{2}{||\omega||}
$$
取倒数得
$$
    min:\frac{||\omega||}{2}
$$
得到最终目标规划问题：
$$
    min:||\omega||^2\\
    s.t.\ \ \ \ y_i(\omega^Tx_i+b)\ge1
$$
最后利用拉格朗日对偶理论，将该问题转化为对偶问题，使用二次规划方法求解，求得最优的$\omega^*$和$b^*$，构造最优分类函数$f(x)$。

在输入空间中，若数据不是线性可分的，支持向量机通过非线性映射$\varnothing:R^n\rightarrow F$将数据映射到某个点积空间$F$,然后在点积空间中执行上述线性算法。在文献中，这一函数称为“核函数”。
### 支持向量机MATLAB程序设计
支持向量机MATLAB程序设计——SVM.m如下：
```
function [x,W,R]=SVM(X0)
for i=1:3
    X(:,i)=(X0(:,i)-mean(X0(:,i)))/std(X0(:,i));
end
[m,n]=size(X);
e=ones(m,1);
D=[X0(:,4)];
B=zeros(m,m);
C=zeros(m,m);
for i =1:m
    B(i,i)=1;
    C(i,i)=D(i,1);
end
A=[-X(:,1).*D,-X(:,2).*D,-X(:,3).*D,D,-B];
b=-e;
f=[0,0,0,0,ones(1,m)];
lb=[-inf,-inf,-inf,-inf,zeros(1,m)]';
x=linprog(f,A,b,[],[],lb);
W=[x(1,1),x(2,1),x(3,1)];
CC=x(4,1);
R1=X*W'-CC;
R2=sign(R1);
R=[R1,R2];
```